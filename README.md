# GPT for R


A GPT code in R language pretrained with Shakespeare text. You can use the trained model to generate Shakespeare-like texts or you can train from scratch with other hyperparameters (vocabulary size, context size, embedding dimension, etc) and other texts. In this repository, it is provided a training with BPE and without BPE (based only with characters like in Karpathy's [video](https://www.youtube.com/watch?v=kCc8FmEb1nY). The Shakespeare text was downloaded [here](https://github.com/karpathy/ng-video-lecture/blob/master/input.txt).

Notice that the generate text is not coherent because the model and the corpus are both small. Increasing the context size, embedding dimension, number of layers and heads, and the size of vocabulary in a subword level, it is expected the generated text will maintain coherence along the words and sentences.

I would like to thanks [Álvaro Kothe](https://github.com/Alvaro-Kothe) for helping me to organize parts of the code in an earlier implementation of GPT for R.

## Dependencies:

- [tokenizers.bpe](https://cran.r-project.org/web/packages/tokenizers.bpe/index.html)
- [torch](https://cran.r-project.org/web/packages/torch/index.html) 
- [luz](https://cran.r-project.org/web/packages/luz/vignettes/get-started.html). Make sure you have installed its version with AMP support. If you have not, set `AMP = FALSE` in the [config.R](https://github.com/AGPatriota/GPT4R/blob/main/config.R) file.

## quick start

If you have a GPU, you can try training this baby GPT model from scratch. You need to define a batch size that fits into your GPU memory (default is 32). Your results might be different for other batch sizes. Before training the model, make sure the number of workers is in order with your machine. Default is 6 workers. In order to train with BPE (Byte-Pair Encoding) or without BPE, open the file `main.R` and set `BPE = TRUE` or `BPE = FALSE`. If you want to train set `Train = TRUE`. If you want to predict tokens set `Run = TRUE`. After setting all hyperparameters in the `main.R` file, run the following in the main folder:

```
source('main.R')
```

The file `Train.R` trains the model. If `BPE = TRUE`, then it will use a vocabulary tokenized by BPE (from the package `tokenizers.bpe`). The file `youtokentome.bpe` contains the tokenized vocabulary (of size 68 tokens) for the Shakespeare text. If `BPE = TRUE`, then it will use a vocabulary with 65 single characters plus a PAD character. This latter procedure is closer to what Karpathy does in his video and the generated text contains proper line breaks which makes the reading a little bit more pleasant.

The Automatic Mixing Precision (AMP)is set as by default in the config.R file to use the Automatic Mixing Precision (AMP). This helps increasing the capacity of your training. If you do not have installed the latest version of the [luz](https://cran.r-project.org/web/packages/luz/vignettes/get-started.html) package, set `AMP = FALSE` in the [config.R](https://github.com/AGPatriota/GPT4R/blob/main/config.R) file.


## Examples of Shakespeare-like texts

An example of 700 tokens generated by setting `BPE = FALSE`, `Train = FALSE` and `Run = TRUE` is given below:

>My lord!
>
>
>Host not my wisdom heart and all through the world
>May up this common counted may his late
>And still majesty that hath some as this sun
>Are between the proof are of lamb:
>And therefore will I see a children's life
>That I shall pluck to my heart
>In the statue, and all their fair means
>Is not true-tuther's face: this is a mile at thing
>Which is the crown?
>
>
>Mayor:
>The summon that they shall not be more to fear:
>That thou hast them and to mark on him.
>
>
>Clown:
>To shame to your country's son: made good Lord Cominius,
>Whose parts for the coin of his eyes
>He dishonour'd to their temperations are slain,
>That makes him for her after lives me
>And better me for them.
>
>Clown:
>'Tis he had as the state o
>
>

An example of 700 tokens generated by setting `BPE = TRUE`, `Train = FALSE` and `Run = TRUE` is given below:


> My lord! what that compasser'd hand that been the hollow of your heads To say 'twere to lay the bear of two well; then I were storm to put the book.'s now thou so that this? what conspire is not this holy life. BUCKINGHAM: Why, she will you do that your love? Boy: The gatisfaction of that part love in prison? And so, now this rude with him; who was the same contrary? KING RICHARD II: What bond he will not stay. much more of these men kings have pass'd To take winter'd in the curses of With her good cause Corioli: who comes there? Messenger: The gods play'd the lady's drunk until that doth hath no more than my state,-- Shall I be from the ground. <UNK>UEEN ELI<UNK>ABETH: No, by my lord, that fatal her is now the door good Rutland he. But hear me in himself, that you could not so love his


### License

MIT
